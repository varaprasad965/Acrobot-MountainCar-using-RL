{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYlPrwBly0sQ",
        "outputId": "4831b74f-2a3b-4056-cbcc-d8ccb024acf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.0\n",
            "    Uninstalling setuptools-67.6.0:\n",
            "      Successfully uninstalled setuptools-67.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-67.6.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "Y29j4VkeSuLZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())\n",
        "print(\"----\")\n",
        "\n",
        "state = env.reset()   \n",
        "\n",
        "print(state)\n",
        "print(\"----\")\n",
        "\n",
        "action = env.action_space.sample()  \n",
        "\n",
        "print(action)\n",
        "print(\"----\")\n",
        "\n",
        "next_state, reward, done, info = env.step(action) \n",
        "\n",
        "print(next_state)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)\n",
        "print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyNuj_GjU8E9",
        "outputId": "201b9061-e89c-44a6-f80e-f6b4561d09a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.01323574  0.17272775 -0.04686959 -0.3551522 ]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden1, hidden2):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden2, action_dim),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        return action_probs, value"
      ],
      "metadata": {
        "id": "l7cO0SM9S-HM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN1 = 128\n",
        "HIDDEN2 = 64\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4\n",
        "SEED = 101"
      ],
      "metadata": {
        "id": "6ML9izxcTB9-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, gamma = GAMMA, seed = SEED):\n",
        "    super(Agent, self).__init__()\n",
        "    self.gamma = gamma\n",
        "    self.ac_model = ActorCritic(state_dim, action_dim, HIDDEN1, HIDDEN2)\n",
        "    self.optimizer = optim.Adam(self.ac_model.parameters(), lr = LR)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "  def sample_action(self, state):\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    action_probs, _ = self.ac_model(state_tensor)\n",
        "    action = np.random.choice(env.action_space.n, p=action_probs.detach().numpy()[0])\n",
        "    return action\n",
        "\n",
        "  def actor_loss(self, action_probs, action, advantage):\n",
        "    actor_loss = -torch.log(action_probs[0, action]) * advantage.detach()\n",
        "    return actor_loss\n",
        "\n",
        "  def critic_loss(self, advantage):\n",
        "    return advantage.pow(2)\n",
        "\n",
        "  def learn(self, states, actions, rewards, state, action, reward):\n",
        "\n",
        "    for t in range(len(states)):\n",
        "      G = 0\n",
        "      for k, r in enumerate(rewards[t:]):\n",
        "          G += self.gamma ** k * r\n",
        "\n",
        "      state_i = torch.FloatTensor(states[t]).unsqueeze(0)\n",
        "      _, value_i = self.ac_model(state_i)\n",
        "      advantage = G - value_i\n",
        "\n",
        "      action_i_probs, _ = self.ac_model(state_i)\n",
        "      actor_loss = -torch.log(action_i_probs[0, actions[t]]) * advantage.detach()\n",
        "      critic_loss = advantage.pow(2)\n",
        "      total_loss = actor_loss + critic_loss\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "      self.optimizer.step()"
      ],
      "metadata": {
        "id": "2QJ4RRlxTFGd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rewards = []\n",
        "n_episodes = []\n",
        "\n",
        "for i in range(10):\n",
        "  env = gym.make('Acrobot-v1')\n",
        "  env._max_episode_steps = 500\n",
        "  env.seed(SEED)\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "  reward_list = []\n",
        "  average_reward_list = []\n",
        "  begin_time = datetime.datetime.now()\n",
        "\n",
        "  agent = Agent(state_dim, action_dim)\n",
        "  num_episodes = 10000\n",
        "\n",
        "  for episode in range(1, num_episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    states, actions, rewards = [], [], []\n",
        "    while not done:\n",
        "      action = agent.sample_action(state)\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      states.append(state)\n",
        "      actions.append(action)\n",
        "      rewards.append(reward)\n",
        "      episode_reward += reward\n",
        "      state = next_state\n",
        "    agent.learn(states, actions, rewards, state, action, reward)\n",
        "    reward_list.append(episode_reward)\n",
        "\n",
        "\n",
        "    if episode < 100:\n",
        "        avg_rew = np.mean(reward_list)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, avg_rew), end=\"\")\n",
        "        avg_100 =  np.mean(reward_list)\n",
        "        if avg_100 > -200.0:\n",
        "          print('Stopped at Episode ',episode)\n",
        "          n_episodes.append(episode)\n",
        "          break\n",
        "    if episode >= 100:\n",
        "      avg_rew = np.mean(reward_list[-100:])\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, avg_rew), end=\"\")\n",
        "    if episode%100 == 0:\n",
        "      avg_rew = np.mean(reward_list[-100:])\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, avg_rew))\n",
        "\n",
        "    if episode >= 100:\n",
        "      avg_100 =  np.mean(reward_list[-100:])\n",
        "      if avg_100 > -200.0:\n",
        "        print('Stopped at Episode ',episode)\n",
        "        n_episodes.append(episode)\n",
        "        break\n",
        "  total_rewards.append(reward_list)\n",
        "  time_taken = datetime.datetime.now() - begin_time\n",
        "  print(time_taken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNmkD5AeTJeY",
        "outputId": "7165fcbd-0666-4d4e-face-ba7ae3944fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10\tAverage Score: -425.40"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def plot_mean_and_variance(mean_array, std_array):\n",
        "    x = np.arange(len(mean_array))\n",
        "    plt.plot(x, mean_array, label='Mean')\n",
        "    plt.fill_between(x, mean_array - std_array, mean_array + std_array, alpha=0.2, label='Standard Deviation')\n",
        "    sns.set_style('darkgrid')\n",
        "    plt.xlabel('Episodes', fontsize = 16)\n",
        "    plt.ylabel('Reward', fontsize = 16)"
      ],
      "metadata": {
        "id": "DXjm0YpoVBIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}